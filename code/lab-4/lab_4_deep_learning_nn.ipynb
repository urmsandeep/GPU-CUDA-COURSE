{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31153,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# üöÄ GPU + CPU Pipelining Demo\n## Understanding How CPU and GPU Work Together in Deep Learning\n\n### Setup Instructions:\n1. **Runtime ‚Üí Change runtime type ‚Üí T4 GPU** \n2. Run all cells in order\n3. Watch the timing comparisons!\n\n---","metadata":{}},{"cell_type":"code","source":"# ============================================\n# CELL 1: Setup and Imports\n# ============================================\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.transforms as transforms\nimport time\nimport numpy as np\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n\nprint(\"=\" * 60)\nprint(\"SETUP CHECK\")\nprint(\"=\" * 60)\nprint(f\"PyTorch Version: {torch.__version__}\")\nprint(f\"CUDA Available: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"GPU Device: {torch.cuda.get_device_name(0)}\")\n    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\nelse:\n    print(\"‚ö†Ô∏è  WARNING: GPU not detected! Please change runtime to T4 GPU\")\nprint(\"=\" * 60)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================\n# CELL 2: Create Synthetic Dataset\n# ============================================\n\nclass SyntheticImageDataset(Dataset):\n    \"\"\"\n    Creates fake images to simulate a real dataset.\n    This simulates loading JPEGs from disk + CPU preprocessing.\n    \"\"\"\n    def __init__(self, num_images=1000, img_size=224, transform=None):\n        self.num_images = num_images\n        self.img_size = img_size\n        self.transform = transform\n        \n    def __len__(self):\n        return self.num_images\n    \n    def __getitem__(self, idx):\n        # Simulate reading & decoding image from disk (CPU work)\n        img = np.random.randint(0, 255, (self.img_size, self.img_size, 3), dtype=np.uint8)\n        img = Image.fromarray(img)\n        \n        # Random label\n        label = idx % 10\n        \n        # Apply transforms (CPU preprocessing!)\n        if self.transform:\n            img = self.transform(img)\n        \n        return img, label\n\nprint(\"‚úì Dataset class created!\")\nprint(\"  This simulates: Disk ‚Üí CPU (load, decode, preprocess)\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================\n# CELL 3: Define a Simple CNN Model\n# ============================================\n\nclass SimpleCNN(nn.Module):\n    \"\"\"Simple CNN for demonstration\"\"\"\n    def __init__(self, num_classes=10):\n        super(SimpleCNN, self).__init__()\n        self.features = nn.Sequential(\n            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n        )\n        self.classifier = nn.Sequential(\n            nn.Linear(64 * 56 * 56, 128),\n            nn.ReLU(inplace=True),\n            nn.Dropout(0.5),\n            nn.Linear(128, num_classes)\n        )\n    \n    def forward(self, x):\n        x = self.features(x)\n        x = x.view(x.size(0), -1)\n        x = self.classifier(x)\n        return x\n\nprint(\"‚úì CNN Model defined!\")\nprint(\"  This will run on GPU for training\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================\n# CELL 4: Training Function with Detailed Timing\n# ============================================\n\ndef train_with_monitoring(model, dataloader, criterion, optimizer, device, num_batches=20):\n    \"\"\"\n    Train and monitor timing of each stage\n    \"\"\"\n    model.train()\n    \n    times = {\n        'data_loading': [],\n        'cpu_to_gpu': [],\n        'forward_backward': [],\n        'total_batch': []\n    }\n    \n    print(f\"\\n{'='*70}\")\n    print(f\"Training {num_batches} batches\")\n    print(f\"{'='*70}\")\n    print(f\"{'Batch':<8} {'Load(ms)':<12} {'Transfer(ms)':<15} {'Train(ms)':<12} {'Total(ms)':<12}\")\n    print(f\"{'-'*70}\")\n    \n    batch_start_time = time.time()\n    \n    for batch_idx, (images, labels) in enumerate(dataloader):\n        if batch_idx >= num_batches:\n            break\n            \n        iter_start = time.time()\n        \n        # Time 1: Data loading (CPU already did this in background)\n        data_load_time = (time.time() - batch_start_time) * 1000\n        \n        # Time 2: Transfer to GPU\n        transfer_start = time.time()\n        images = images.to(device, non_blocking=True)\n        labels = labels.to(device, non_blocking=True)\n        torch.cuda.synchronize()\n        transfer_time = (time.time() - transfer_start) * 1000\n        \n        # Time 3: GPU Training\n        train_start = time.time()\n        optimizer.zero_grad()\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        torch.cuda.synchronize()\n        train_time = (time.time() - train_start) * 1000\n        \n        total_time = (time.time() - iter_start) * 1000\n        \n        times['data_loading'].append(data_load_time)\n        times['cpu_to_gpu'].append(transfer_time)\n        times['forward_backward'].append(train_time)\n        times['total_batch'].append(total_time)\n        \n        print(f\"{batch_idx:<8} {data_load_time:>10.2f}  {transfer_time:>13.2f}  {train_time:>10.2f}  {total_time:>10.2f}\")\n        \n        batch_start_time = time.time()\n    \n    return times\n\nprint(\"‚úì Training function created!\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---\n## üü¢ Experiment 1: WITH Pipelining (The Good Way)\n\n**Setup:** `num_workers=2` ‚Üí CPU workers prepare batches ahead\n\n**Expected:** GPU is always busy, minimal waiting\n\n---","metadata":{}},{"cell_type":"code","source":"# ============================================\n# EXPERIMENT 1: WITH PIPELINING\n# ============================================\n\nprint(\"=\"*70)\nprint(\"EXPERIMENT 1: WITH PIPELINING (num_workers=2)\")\nprint(\"=\"*70)\nprint(\"‚úÖ CPU workers prepare batches while GPU trains\")\nprint()\n\n# Transforms (all CPU work!)\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.RandomHorizontalFlip(),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], \n                        std=[0.229, 0.224, 0.225])\n])\n\n# Create dataset\ndataset_good = SyntheticImageDataset(num_images=500, transform=transform)\n\n# DataLoader WITH workers (PIPELINING!)\ndataloader_good = DataLoader(\n    dataset_good,\n    batch_size=32,\n    shuffle=True,\n    num_workers=2,          # ‚Üê CPU workers!\n    pin_memory=True,        # ‚Üê Fast GPU transfer\n    prefetch_factor=2,      # ‚Üê Prepare 2 batches ahead\n    persistent_workers=True\n)\n\n# Create model\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel_good = SimpleCNN(num_classes=10).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model_good.parameters(), lr=0.001)\n\n# Train!\ntimes_good = train_with_monitoring(\n    model_good, dataloader_good, criterion, optimizer, device, num_batches=20\n)\n\nprint(f\"\\n{'='*70}\")\nprint(\"EXPERIMENT 1 RESULTS\")\nprint(f\"{'='*70}\")\nprint(f\"Average Data Loading: {np.mean(times_good['data_loading']):.2f} ms\")\nprint(f\"Average GPU Transfer:  {np.mean(times_good['cpu_to_gpu']):.2f} ms\")\nprint(f\"Average GPU Training:  {np.mean(times_good['forward_backward']):.2f} ms\")\nprint(f\"Average Total/Batch:   {np.mean(times_good['total_batch']):.2f} ms\")\nprint(f\"{'='*70}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---\n## üî¥ Experiment 2: WITHOUT Pipelining (The Bad Way)\n\n**Setup:** `num_workers=0` ‚Üí No background workers\n\n**Expected:** GPU waits for CPU, much slower!\n\n---","metadata":{}},{"cell_type":"code","source":"# ============================================\n# EXPERIMENT 2: WITHOUT PIPELINING\n# ============================================\n\nprint(\"=\"*70)\nprint(\"EXPERIMENT 2: WITHOUT PIPELINING (num_workers=0)\")\nprint(\"=\"*70)\nprint(\"‚ùå Main thread does everything - GPU waits!\")\nprint()\n\n# Same dataset\ndataset_bad = SyntheticImageDataset(num_images=500, transform=transform)\n\n# DataLoader WITHOUT workers (NO PIPELINING!)\ndataloader_bad = DataLoader(\n    dataset_bad,\n    batch_size=32,\n    shuffle=True,\n    num_workers=0,  # ‚Üê No workers! Sequential!\n    pin_memory=False\n)\n\n# Fresh model\nmodel_bad = SimpleCNN(num_classes=10).to(device)\noptimizer = optim.Adam(model_bad.parameters(), lr=0.001)\n\n# Train!\ntimes_bad = train_with_monitoring(\n    model_bad, dataloader_bad, criterion, optimizer, device, num_batches=20\n)\n\nprint(f\"\\n{'='*70}\")\nprint(\"EXPERIMENT 2 RESULTS\")\nprint(f\"{'='*70}\")\nprint(f\"Average Data Loading: {np.mean(times_bad['data_loading']):.2f} ms\")\nprint(f\"Average GPU Transfer:  {np.mean(times_bad['cpu_to_gpu']):.2f} ms\")\nprint(f\"Average GPU Training:  {np.mean(times_bad['forward_backward']):.2f} ms\")\nprint(f\"Average Total/Batch:   {np.mean(times_bad['total_batch']):.2f} ms\")\nprint(f\"{'='*70}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---\n## üìä Comparison & Visualization\n---","metadata":{}},{"cell_type":"code","source":"# ============================================\n# COMPARISON & VISUALIZATION\n# ============================================\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"üéØ FINAL COMPARISON\")\nprint(\"=\"*70)\n\navg_time_good = np.mean(times_good['total_batch'])\navg_time_bad = np.mean(times_bad['total_batch'])\nspeedup = avg_time_bad / avg_time_good\n\nprint(f\"\\nWITH Pipelining (num_workers=2):    {avg_time_good:.2f} ms/batch\")\nprint(f\"WITHOUT Pipelining (num_workers=0): {avg_time_bad:.2f} ms/batch\")\nprint(f\"\\nüöÄ SPEEDUP: {speedup:.2f}x FASTER with pipelining!\")\nprint(f\"üíæ Time saved: {avg_time_bad - avg_time_good:.2f} ms per batch\")\nprint(f\"üìä For 1000 batches: {(avg_time_bad - avg_time_good) * 1000 / 1000:.1f} seconds saved!\")\n\n# Visualization\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Plot 1: Breakdown\ncategories = ['Data Load', 'GPU Transfer', 'GPU Train', 'Total']\ngood_times = [\n    np.mean(times_good['data_loading']),\n    np.mean(times_good['cpu_to_gpu']),\n    np.mean(times_good['forward_backward']),\n    np.mean(times_good['total_batch'])\n]\nbad_times = [\n    np.mean(times_bad['data_loading']),\n    np.mean(times_bad['cpu_to_gpu']),\n    np.mean(times_bad['forward_backward']),\n    np.mean(times_bad['total_batch'])\n]\n\nx = np.arange(len(categories))\nwidth = 0.35\n\nbars1 = axes[0].bar(x - width/2, good_times, width, label='WITH Pipelining', color='green', alpha=0.7)\nbars2 = axes[0].bar(x + width/2, bad_times, width, label='WITHOUT Pipelining', color='red', alpha=0.7)\n\naxes[0].set_xlabel('Stage', fontsize=12)\naxes[0].set_ylabel('Time (ms)', fontsize=12)\naxes[0].set_title('Timing Breakdown: WITH vs WITHOUT Pipelining', fontsize=14, fontweight='bold')\naxes[0].set_xticks(x)\naxes[0].set_xticklabels(categories)\naxes[0].legend()\naxes[0].grid(True, alpha=0.3)\n\nfor bars in [bars1, bars2]:\n    for bar in bars:\n        height = bar.get_height()\n        axes[0].text(bar.get_x() + bar.get_width()/2., height,\n                    f'{height:.1f}',\n                    ha='center', va='bottom', fontsize=9)\n\n# Plot 2: Speed comparison\naxes[1].barh(['WITHOUT\\nPipelining', 'WITH\\nPipelining'], \n            [avg_time_bad, avg_time_good],\n            color=['red', 'green'], alpha=0.7, height=0.6)\naxes[1].set_xlabel('Time per Batch (ms)', fontsize=12)\naxes[1].set_title(f'Speed Comparison: {speedup:.2f}x Faster!', fontsize=14, fontweight='bold')\naxes[1].grid(True, alpha=0.3, axis='x')\n\nfor i, v in enumerate([avg_time_bad, avg_time_good]):\n    axes[1].text(v + 2, i, f'{v:.1f} ms', va='center', fontsize=11, fontweight='bold')\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\n‚úì Visualization complete!\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---\n## üîç Check GPU Utilization\n---","metadata":{}},{"cell_type":"code","source":"# Check GPU status\n!nvidia-smi","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---\n## üéì Summary: Key Learnings\n\n### ‚úÖ What We Learned:\n\n1. **Images DO go through CPU first** (load, decode, preprocess)\n2. **BUT GPU is NOT idle** - it works on previous batch!\n3. **Pipelining = Parallelism**: CPU and GPU work simultaneously on different batches\n4. **DataLoader workers** enable this pipelining automatically\n5. **Result**: Significant speedup (typically 1.5x - 3x faster!)\n\n### üîß Best Practices:\n\n```python\nDataLoader(\n    dataset,\n    num_workers=4,          # ‚Üê Use 4-8 workers\n    pin_memory=True,        # ‚Üê Faster GPU transfer\n    prefetch_factor=2,      # ‚Üê Prepare batches ahead\n    persistent_workers=True # ‚Üê Keep workers alive\n)\n```\n\n### üìä The Pipeline:\n\n```\nWorker 1: Loading Batch N+1\nWorker 2: Decoding Batch N+2  } All happening\nWorker 3: Augmenting Batch N+3} simultaneously!\nGPU:      Training Batch N     }\n```\n\n---\n\n### üéØ Remember:\n\n**Images MUST go through CPU, but with smart pipelining, the GPU never waits!**\n\n---","metadata":{}}]}